{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "236853f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, Row\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c727036",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# Add the path to your module's directory\n",
    "sys.path.append('/home/vladyslav_podrazhanskyi/projects/PERSONAL/python/learn_spark')\n",
    "\n",
    "# Now you can import your \n",
    "from my_code.utils import ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7249dcb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/12 16:52:58 WARN Utils: Your hostname, EPUAKHAW05DF resolves to a loopback address: 127.0.1.1; using 192.168.100.3 instead (on interface wlp0s20f3)\n",
      "24/02/12 16:52:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/12 16:52:59 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7fea46aa6510>\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76358569-e25f-4f1c-901f-299d9aeaca32",
   "metadata": {},
   "source": [
    "You can manually create a PySpark DataFrame using \n",
    "toDF() and createDataFrame() methods, \n",
    "both these function takes different signatures in order to create DataFrame from existing RDD, list, and DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bb0f531-a673-4d86-8945-f91a06667908",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to create a DataFrame from a list we need the data hence, first, let’s create the data and the columns that are needed.\n",
    "\n",
    "columns = [\"language\",\"users_count\"]\n",
    "data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de963d6f-9021-40be-a491-4810c83c2390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:289\n",
      "<class 'pyspark.rdd.RDD'>\n"
     ]
    }
   ],
   "source": [
    "# 0. Create RDD\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "print(rdd)\n",
    "print(type(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60201773-ffc0-4213-a488-ef73a13c0f5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _1: string (nullable = true)\n",
      " |-- _2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.1 Using toDF() function\n",
    "PySpark RDD’s toDF() method is used to create a DataFrame from the existing RDD. Since RDD doesn’t have columns, \n",
    "the DataFrame is created with default column names “_1” and “_2” as we have two columns.\n",
    "\"\"\"\n",
    "\n",
    "# convert rdd to spark dataframe without column names using toDF \n",
    "\n",
    "dfFromRDD1 = rdd.toDF()      # can be added column names with \n",
    "dfFromRDD1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88d0d5b3-2256-4e57-b181-b8e3346969c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# convert rdd to spark dataframe with column names using toDF(*columns) \n",
    "\n",
    "dfFromRDD2 = rdd.toDF(columns)    # without * for RDD\n",
    "dfFromRDD2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "204fd44c-2ce0-4f58-b200-857501fb1f7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "1.2 Using createDataFrame() from SparkSession\n",
    "Using createDataFrame() from SparkSession is another way to create manually \n",
    "and it takes rdd object as an argument. and chain with toDF() to specify name to the columns.\n",
    "\"\"\"\n",
    "\n",
    "# convert rdd with column names using createDataFrame \n",
    "# createDataFrame method can accept data both rdd and list (see 2.1.) \n",
    "\n",
    "\n",
    "dfFromRDD3 = spark.createDataFrame(rdd).toDF(*columns)   # * in *columns because this DF, not RDD\n",
    "\n",
    "dfFromRDD3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "206095e9-8bd7-465c-9734-dcc198a42e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2. Create DataFrame from List Collection\n",
    "\n",
    "In this section, we will see how to create PySpark DataFrame from a list. \n",
    "These examples would be similar to what we have seen in the above section with RDD, \n",
    "but we use the list data object instead of “rdd” object to create DataFrame.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "2.1 Using createDataFrame() from SparkSession\n",
    "Calling createDataFrame() from SparkSession is another way to create PySpark DataFrame manually, \n",
    "it takes a list object as an argument. and chain with toDF() to specify names to the columns.\n",
    "\"\"\"\n",
    "\n",
    "# df from list\n",
    "\n",
    "dfFromData = spark.createDataFrame(data).toDF(*columns)   # spark.createDataFrame(data, columns)\n",
    "dfFromData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64c2e0c9-ac5a-49a9-9913-1375975abd65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Row('Java', '20000')>, <Row('Python', '100000')>, <Row('Scala', '3000')>]\n",
      "+--------+-----------+\n",
      "|language|users_count|\n",
      "+--------+-----------+\n",
      "|    Java|      20000|\n",
      "|  Python|     100000|\n",
      "|   Scala|       3000|\n",
      "+--------+-----------+\n",
      "\n",
      "root\n",
      " |-- language: string (nullable = true)\n",
      " |-- users_count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.2 Using createDataFrame() with the Row type\n",
    "createDataFrame() has another signature in PySpark which takes the collection of Row type and schema \n",
    "for column names as arguments.\n",
    "To use this first we need to convert our “data” object from the list to list of Row.\n",
    "\"\"\"\n",
    "\n",
    "# data = [(\"Java\", \"20000\"), (\"Python\", \"100000\"), (\"Scala\", \"3000\")]\n",
    "\n",
    "rowData = map(lambda x: Row(*x), data)\n",
    "print(list(rowData))\n",
    "\n",
    "dfFromRowData = spark.createDataFrame(data, columns)\n",
    "dfFromRowData.show()\n",
    "dfFromRowData.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b8ff1a0-190f-4043-9cbd-976c7d17423f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- middlename: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|   id|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|    James|          |   Smith|36636|     M|  3000|\n",
      "|  Michael|      Rose|        |40288|     M|  4000|\n",
      "|   Robert|          |Williams|42114|     M|  4000|\n",
      "|    Maria|      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "2.3 Create DataFrame with schema\n",
    "If you wanted to specify the column names along with their data types, \n",
    "you should create the StructType schema first and then assign this while creating a DataFrame.\n",
    "\"\"\"\n",
    "\n",
    "data2 = [\n",
    "    (\"James\",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "    (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "    (\"Robert\",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "    (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "    (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)\n",
    "]\n",
    "\n",
    "schema = T.StructType([ \n",
    "    T.StructField(\"firstname\", T.StringType(),True), \n",
    "    T.StructField(\"middlename\", T.StringType(),True), \n",
    "    T.StructField(\"lastname\", T.StringType(),True), \n",
    "    T.StructField(\"id\", T.StringType(), True), \n",
    "    T.StructField(\"gender\", T.StringType(), True), \n",
    "    T.StructField(\"salary\", T.IntegerType(), True) \n",
    "  ])\n",
    "\n",
    "dfData2 = spark.createDataFrame(data2, schema)\n",
    "dfData2.printSchema()\n",
    "dfData2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62be0bd-4775-4e1e-a2ed-28b0ea60fda2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,incorrectly_encoded_metadata,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
