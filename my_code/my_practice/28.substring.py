from decimal import Decimal

from pyspark.sql import SparkSession, DataFrame, Row
import pyspark.sql.types as st
import pyspark.sql.functions as sf

spark = (
    SparkSession
    .builder
    .master("local[*]")
    .getOrCreate()
)

# spark.sparkContext.setLogLevel("INFO")

schema = st.StructType([
    st.StructField("id", st.IntegerType(), True),
    st.StructField("col1", st.FloatType(), True),
    st.StructField("col2", st.FloatType(), True),
    st.StructField("col3", st.FloatType(), True),
])


cur_df = spark.createDataFrame(
    [
        (1, 5.0, 5.1, 5.2),
        (2, 7.0, 0.0, 7.2),
        (3, 9.0, None, 9.2),
        (4, 0.0, 0.1, 1.0),
        (5, None, 0.2, 2.0),
        (6, 0.0, None, 3.0),
        (7, 0.0, 0.0, 3.5),
        (8, None, None, 4.0),
        (9, None, 0.0, 4.1),

    ],
    schema=schema
)

cur_df.show()

case_df = cur_df.withColumnRenamed(
    'col3',
    'old_col3'
).withColumn(
    'col3',
    sf.when(
        sf.col("col1").isNotNull()
        & (sf.col('col1') != 0),
        sf.col('col1')
    ).otherwise(
        sf.when(
            sf.col("col2").isNotNull()
            & (sf.col('col2') != 0),
            sf.col('col2')
        ).otherwise(sf.col('old_col3'))
    )
)

case_df.show()


# pyspark.sql.utils.IllegalArgumentException: when() can only be applied on a Column previously generated by when() function
# case2_df = cur_df.withColumnRenamed(
#     'col3',
#     'old_col3'
# ).withColumn(
#     'col3',
#     sf.when(
#         sf.col("col1").isNotNull()
#         & (sf.col('col1') != 0),
#         sf.col('col1')
#     ).sf.when(
#         sf.col("col2").isNotNull()
#         & (sf.col('col2') != 0),
#         sf.col('col2')
#     ).otherwise(sf.col('old_col3'))
# )
#
# case2_df.show()
