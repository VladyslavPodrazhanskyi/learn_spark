{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40cfaa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "483d1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ed933ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/vladyslav_podrazhanskyi/projects/PERSONAL/python/learn_spark')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Add the path to your module's directory\n",
    "sys.path.append('/home/vladyslav_podrazhanskyi/projects/PERSONAL/python/learn_spark')\n",
    "\n",
    "# Now you can import your \n",
    "from my_code.utils import ROOT\n",
    "\n",
    "ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd78d307",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/13 16:17:44 INFO SparkContext: Running Spark version 3.5.0\n",
      "24/02/13 16:17:44 INFO SparkContext: OS info Linux, 6.5.0-17-generic, amd64\n",
      "24/02/13 16:17:44 INFO SparkContext: Java version 1.8.0_392\n",
      "24/02/13 16:17:44 INFO ResourceUtils: ==============================================================\n",
      "24/02/13 16:17:44 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "24/02/13 16:17:44 INFO ResourceUtils: ==============================================================\n",
      "24/02/13 16:17:44 INFO SparkContext: Submitted application: pyspark-shell\n",
      "24/02/13 16:17:44 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "24/02/13 16:17:44 INFO ResourceProfile: Limiting resource is cpu\n",
      "24/02/13 16:17:44 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "24/02/13 16:17:44 INFO SecurityManager: Changing view acls to: vladyslav_podrazhanskyi\n",
      "24/02/13 16:17:44 INFO SecurityManager: Changing modify acls to: vladyslav_podrazhanskyi\n",
      "24/02/13 16:17:44 INFO SecurityManager: Changing view acls groups to: \n",
      "24/02/13 16:17:44 INFO SecurityManager: Changing modify acls groups to: \n",
      "24/02/13 16:17:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: vladyslav_podrazhanskyi; groups with view permissions: EMPTY; users with modify permissions: vladyslav_podrazhanskyi; groups with modify permissions: EMPTY\n",
      "24/02/13 16:17:44 INFO Utils: Successfully started service 'sparkDriver' on port 41835.\n",
      "24/02/13 16:17:44 INFO SparkEnv: Registering MapOutputTracker\n",
      "24/02/13 16:17:44 INFO SparkEnv: Registering BlockManagerMaster\n",
      "24/02/13 16:17:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "24/02/13 16:17:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "24/02/13 16:17:44 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "24/02/13 16:17:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e18ef428-0092-4d3e-b748-1a007896a0ae\n",
      "24/02/13 16:17:44 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "24/02/13 16:17:44 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "24/02/13 16:17:44 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI\n",
      "24/02/13 16:17:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.\n",
      "24/02/13 16:17:44 INFO Executor: Starting executor ID driver on host 192.168.100.3\n",
      "24/02/13 16:17:44 INFO Executor: OS info Linux, 6.5.0-17-generic, amd64\n",
      "24/02/13 16:17:44 INFO Executor: Java version 1.8.0_392\n",
      "24/02/13 16:17:44 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "24/02/13 16:17:44 INFO Executor: Created or updated repl class loader org.apache.spark.util.MutableURLClassLoader@49c029f9 for default.\n",
      "24/02/13 16:17:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41331.\n",
      "24/02/13 16:17:44 INFO NettyBlockTransferService: Server created on 192.168.100.3:41331\n",
      "24/02/13 16:17:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "24/02/13 16:17:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.100.3, 41331, None)\n",
      "24/02/13 16:17:44 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.100.3:41331 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.100.3, 41331, None)\n",
      "24/02/13 16:17:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.100.3, 41331, None)\n",
      "24/02/13 16:17:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.100.3, 41331, None)\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eebf7258-9c92-439e-8310-0ddc8fff03ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"WARN\")   # \"INFO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d2927bbd-332c-490a-9d15-73a8c13f0008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24\n",
      "[Database(name='default', catalog='spark_catalog', description='default database', locationUri='file:/home/vladyslav_podrazhanskyi/projects/PERSONAL/python/learn_spark/my_code/my_practice/spark-warehouse')]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# spark catalog\n",
    "catalog = spark.catalog\n",
    "pprint(catalog.__sizeof__())\n",
    "pprint(catalog.listDatabases())\n",
    "pprint(catalog.listTables())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb0e66d",
   "metadata": {},
   "source": [
    "UI: http://127.0.0.1:4040/jobs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "06e0e762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# spark reader\n",
    "df = spark.read.format(\"json\").load(f\"{ROOT}/source_data/flight-data/json/\")\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1882d237",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df_renamed = df.withColumnRenamed(\"ORIGIN_COUNTRY_NAME\", \"renamed_ORIGIN_COUNTRY_NAME\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "36402989",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|renamed_ORIGIN_COUNTRY_NAME|count|\n",
      "+--------------------+---------------------------+-----+\n",
      "|       United States|                    Romania|   15|\n",
      "|       United States|                    Croatia|    1|\n",
      "|       United States|                    Ireland|  344|\n",
      "|               Egypt|              United States|   15|\n",
      "|       United States|                      India|   62|\n",
      "|       United States|                  Singapore|    1|\n",
      "|       United States|                    Grenada|   62|\n",
      "|          Costa Rica|              United States|  588|\n",
      "|             Senegal|              United States|   40|\n",
      "|             Moldova|              United States|    1|\n",
      "|       United States|               Sint Maarten|  325|\n",
      "|       United States|           Marshall Islands|   39|\n",
      "|              Guyana|              United States|   64|\n",
      "|               Malta|              United States|    1|\n",
      "|            Anguilla|              United States|   41|\n",
      "|             Bolivia|              United States|   30|\n",
      "|       United States|                   Paraguay|    6|\n",
      "|             Algeria|              United States|    4|\n",
      "|Turks and Caicos ...|              United States|  230|\n",
      "|       United States|                  Gibraltar|    1|\n",
      "+--------------------+---------------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "1502\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- renamed_ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n",
      "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('renamed_ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])\n"
     ]
    }
   ],
   "source": [
    "df_renamed.show()\n",
    "print(df_renamed.count())  # 1502\n",
    "df_renamed.printSchema()\n",
    "print(df_renamed.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "270b8d94-539f-46b3-adbb-dc53a48f6063",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_with_five\n",
      "+--------------------+-------------------+-----+----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|five|\n",
      "+--------------------+-------------------+-----+----+\n",
      "|       United States|            Romania|   15| 5.0|\n",
      "|       United States|            Croatia|    1| 5.0|\n",
      "|       United States|            Ireland|  344| 5.0|\n",
      "|               Egypt|      United States|   15| 5.0|\n",
      "|       United States|              India|   62| 5.0|\n",
      "|       United States|          Singapore|    1| 5.0|\n",
      "|       United States|            Grenada|   62| 5.0|\n",
      "|          Costa Rica|      United States|  588| 5.0|\n",
      "|             Senegal|      United States|   40| 5.0|\n",
      "|             Moldova|      United States|    1| 5.0|\n",
      "|       United States|       Sint Maarten|  325| 5.0|\n",
      "|       United States|   Marshall Islands|   39| 5.0|\n",
      "|              Guyana|      United States|   64| 5.0|\n",
      "|               Malta|      United States|    1| 5.0|\n",
      "|            Anguilla|      United States|   41| 5.0|\n",
      "|             Bolivia|      United States|   30| 5.0|\n",
      "|       United States|           Paraguay|    6| 5.0|\n",
      "|             Algeria|      United States|    4| 5.0|\n",
      "|Turks and Caicos ...|      United States|  230| 5.0|\n",
      "|       United States|          Gibraltar|    1| 5.0|\n",
      "+--------------------+-------------------+-----+----+\n",
      "only showing top 20 rows\n",
      "\n",
      "dif1_schema\n",
      "{StructField('five', DoubleType(), False)}\n",
      "dif1_schema\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "df_with_five = df.withColumn(\"five\", sf.lit(5.0))\n",
    "print(\"df_with_five\")\n",
    "df_with_five.show()\n",
    "print(\"dif1_schema\")\n",
    "print(set(df_with_five.schema) - set(df.schema))\n",
    "print(\"dif2_schema\")\n",
    "print(set(df.schema) - set(df_with_five.schema))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9eb2ab0b-3147-4c5e-8444-f94f5bd9e7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   15|\n",
      "|    1|\n",
      "|  344|\n",
      "+-----+\n",
      "only showing top 3 rows\n",
      "\n",
      "MapPartitionsRDD[225] at javaToPython at NativeMethodAccessorImpl.java:0\n",
      "Row(count=370002)\n",
      "370002\n",
      "+------+\n",
      "|   max|\n",
      "+------+\n",
      "|370002|\n",
      "+------+\n",
      "\n",
      "[Row(max=370002)]\n",
      "370002\n"
     ]
    }
   ],
   "source": [
    "df.select('count').show(3)\n",
    "print(df.select('count').rdd)           # MapPartitionsRDD[27] at javaToPython at NativeMethodAccessorImpl.java:0\n",
    "print(df.select('count').rdd.max())     # Row(count=370002)\n",
    "print(df.select('count').rdd.max()[0])  # 370002\n",
    "\n",
    "df.select(sf.max('count').alias('max')).show()\n",
    "print(df.select(sf.max('count').alias('max')).collect()) # [Row(max=370002)]\n",
    "print(df.select(sf.max('count').alias('max')).collect()[0]['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5a4dac26-2413-4f1c-b606-633fb28508ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|            Romania|   15|\n",
      "|            Croatia|    1|\n",
      "|            Ireland|  344|\n",
      "|      United States|   15|\n",
      "|              India|   62|\n",
      "+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-------------------+-----+\n",
      "|ORIGIN_COUNTRY_NAME|count|\n",
      "+-------------------+-----+\n",
      "|            Romania|   15|\n",
      "|            Croatia|    1|\n",
      "|            Ireland|  344|\n",
      "|      United States|   15|\n",
      "|              India|   62|\n",
      "+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|\n",
      "|            Egypt|            Egypt|            Egypt|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df.select(\"ORIGIN_COUNTRY_NAME\", \"count\").show(5)\n",
    "df.selectExpr(\"ORIGIN_COUNTRY_NAME\", \"count\").show(5)\n",
    "\n",
    "df.select(\n",
    "    \"DEST_COUNTRY_NAME\",\n",
    "    sf.expr(\"DEST_COUNTRY_NAME\"),\n",
    "    sf.col(\"DEST_COUNTRY_NAME\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60500f25",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "advantage of F.expr in comparison with col\n",
    "df.select(F.expr(\"DEST_COUNTRY_NAME AS destination\")).show(2)\n",
    "df.select(F.col(\"DEST_COUNTRY_NAME\").alias(\"destination\")).show(2)\n",
    "\n",
    "# selectExpr !!!\n",
    "\"\"\"\n",
    "Because select followed by a series of expr is such a common pattern, Spark has a shorthand\n",
    "for doing this efficiently: selectExpr. This is probably the most convenient interface for\n",
    "everyday use\n",
    "This opens up the true power of Spark. We can treat selectExpr as a simple way to build up\n",
    "complex expressions that create new DataFrames.\n",
    "\"\"\"\n",
    "df.selectExpr(\"DEST_COUNTRY_NAME as newColumnName\", \"DEST_COUNTRY_NAME\").show(5)\n",
    "\n",
    "df.selectExpr(\n",
    "    \"*\",\n",
    "    \"(DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\") \\\n",
    "    .show(100)\n",
    "\n",
    "\n",
    "# aggregations over the entire DataFrame\n",
    "\n",
    "df.selectExpr(\"avg(count)\", \"count(distinct(DEST_COUNTRY_NAME))\").show(2)\n",
    "\n",
    "\n",
    "\n",
    "print(\"sqlWay\")\n",
    "\n",
    "# temporary view for query with SQL\n",
    "df.createOrReplaceTempView(\"dfTable\")\n",
    "\n",
    "sqlWayWithinCountry = spark.sql(\n",
    "    \"\"\"\n",
    "    SELECT *,\n",
    "    (DEST_COUNTRY_NAME = ORIGIN_COUNTRY_NAME) as withinCountry\n",
    "    FROM dfTable\n",
    "    \"\"\"\n",
    ").filter(sf.col('withinCountry')).show(12)\n",
    "\n",
    "pprint(catalog.listTables()) # [Table(name='dftable', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc963a9",
   "metadata": {},
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "SELECT\n",
    "DEST_COUNTRY_NAME, Sum(count) as sum\n",
    "FROM dfTable\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER by sum DESC\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd3e0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(600)\n",
    "spar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206ba2e6-1a48-4502-a1d8-17185eebcfda",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
